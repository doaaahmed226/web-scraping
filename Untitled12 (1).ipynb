{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RzGX60p27CI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# Fetch the HTML content of the page\n",
        "url = \"https://www.baraasallout.com/test.html\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract headings and text content\n",
        "headings = soup.find_all(['h1', 'h2'])\n",
        "paragraphs = soup.find_all('p')\n",
        "list_items = soup.find_all('li')\n",
        "\n",
        "# Collect data\n",
        "data = []\n",
        "for heading in headings:\n",
        "    data.append([\"Heading\", heading.get_text()])\n",
        "for p in paragraphs:\n",
        "    data.append([\"Paragraph\", p.get_text()])\n",
        "for li in list_items:\n",
        "    data.append([\"List Item\", li.get_text()])\n",
        "\n",
        "# Write to CSV\n",
        "with open('Extract_Text_Data.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Type', 'Content'])\n",
        "    writer.writerows(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the table has specific classes or attributes\n",
        "table = soup.find('table')  # Adjust selector if necessary\n",
        "rows = table.find_all('tr')\n",
        "\n",
        "# Extract relevant data (e.g., product name, price, stock status)\n",
        "table_data = []\n",
        "for row in rows:\n",
        "    cols = row.find_all('td')\n",
        "    if len(cols) >= 3:  # Ensure there are enough columns\n",
        "        product_name = cols[0].get_text()\n",
        "        price = cols[1].get_text()\n",
        "        stock_status = cols[2].get_text()\n",
        "        table_data.append([product_name, price, stock_status])\n",
        "\n",
        "# Write to CSV\n",
        "with open('Extract_Table_Data.csv', mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['Product Name', 'Price', 'Stock Status'])\n",
        "    writer.writerows(table_data)\n"
      ],
      "metadata": {
        "id": "0iFxYmbE3yKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M4eKAVta362S",
        "outputId": "60189a6c-58cb-47eb-9563-14ebc9a742c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product information has been saved to 'Product_Information.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product_cards = soup.find_all('div', class_='product-card')\n",
        "products = []\n",
        "\n",
        "for card in product_cards:\n",
        "    product = {\n",
        "        'Book Title': card.find('h2').text.strip() if card.find('h2') else \"No title\",\n",
        "        'Price': card.find('span', class_='price').text.strip() if card.find('span', class_='price') else \"No price\",\n",
        "        'Stock Availability': card.find('span', class_='stock-status').text.strip() if card.find('span', class_='stock-status') else \"No stock info\",\n",
        "        'Button Text': card.find('button').text.strip() if card.find('button') else \"No button text\"\n",
        "    }\n",
        "    products.append(product)\n",
        "\n",
        "with open('Product_Information.json', 'w') as json_file:\n",
        "    json.dump(products, json_file, indent=4)\n",
        "\n",
        "print(\"Product information has been saved to 'Product_Information.json'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tmK-wqC-M4q",
        "outputId": "2252c17d-37b7-432e-c68b-b827b8be575d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product information has been saved to 'Product_Information.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# URL of the page\n",
        "url = \"https://www.baraasallout.com/test.html\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the page content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find all forms in the page\n",
        "forms = soup.find_all('form')\n",
        "\n",
        "# List to store extracted form data\n",
        "form_data = []\n",
        "\n",
        "# Loop through each form and extract input field details\n",
        "for form in forms:\n",
        "    form_info = []\n",
        "    # Find all input fields in the form\n",
        "    inputs = form.find_all('input')\n",
        "\n",
        "    for input_tag in inputs:\n",
        "        # Extracting field name, type, and default value (if available)\n",
        "        field_name = input_tag.get('name', 'N/A')\n",
        "        field_type = input_tag.get('type', 'text')  # Default to 'text' if type is missing\n",
        "        default_value = input_tag.get('value', 'N/A')\n",
        "\n",
        "        # Create a dictionary for the input field\n",
        "        input_field = {\n",
        "            'Field Name': field_name,\n",
        "            'Input Type': field_type,\n",
        "            'Default Value': default_value\n",
        "        }\n",
        "        form_info.append(input_field)\n",
        "\n",
        "    # Append form information to the list\n",
        "    form_data.append(form_info)\n",
        "\n",
        "# Save the extracted form details to a JSON file\n",
        "with open('Form_Details.json', 'w') as json_file:\n",
        "    json.dump(form_data, json_file, indent=4)\n",
        "\n",
        "print(\"Form details have been saved to 'Form_Details.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUaI5qio-r05",
        "outputId": "c005bd7c-c4d8-427e-a119-6e555330a313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Form details have been saved to 'Form_Details.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# URL of the page\n",
        "url = \"https://www.baraasallout.com/test.html\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the page content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# List to store extracted links and video sources\n",
        "links_and_videos = []\n",
        "\n",
        "# Extract all <a> tags and their href attribute (hyperlinks)\n",
        "for a_tag in soup.find_all('a', href=True):\n",
        "    link_info = {\n",
        "        'Link Text': a_tag.get_text(strip=True),\n",
        "        'URL': a_tag['href']\n",
        "    }\n",
        "    links_and_videos.append(link_info)\n",
        "\n",
        "# Extract all <iframe> tags and their src attribute (video links)\n",
        "for iframe_tag in soup.find_all('iframe', src=True):\n",
        "    video_info = {\n",
        "        'Video Source': iframe_tag['src']\n",
        "    }\n",
        "    links_and_videos.append(video_info)\n",
        "\n",
        "# Save the extracted links and videos to a JSON file\n",
        "with open('Links_and_Videos.json', 'w') as json_file:\n",
        "    json.dump(links_and_videos, json_file, indent=4)\n",
        "\n",
        "print(\"Links and multimedia have been saved to 'Links_and_Videos.json'\")\n"
      ],
      "metadata": {
        "id": "kRLMeveg_A24",
        "outputId": "6cbb2280-eb53-4c83-c633-7fc2f5682ab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Links and multimedia have been saved to 'Links_and_Videos.json'\n"
          ]
        }
      ]
    }
  ]
}